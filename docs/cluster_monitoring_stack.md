# Time series data monitoring

POC can deploy:

  * Prometheus for scraping various metrics about the OpenShift installation via 
  the Prometheus operator.
  * Thanos for enabling long term Prometheus data retention.
  * kube-state-metrics for collecting various cluster related metrics.
  * Heketi metrics exporter for collecting GlusterFS nodes metrics.
  * Node exporter for fetching low level Linux metrics from OpenShift nodes.
  * Grafana for visualizing the data from Prometheus and InfluxDB.

The playbook that deploys all of these is `playbooks/deploy_monitoring.yml`
which runs during the post-install phase.

This document is intended as a high level overview of the time series data
setup. For more detail, please refer to the code and configuration referenced
here.

## Overview

Prometheus scrapes data from various sources (kube-state-metrics, heketi metrics 
exporter, node exporter) and is configured to run with Thanos as sidecar container. 
Thanos then uploads the data generated by Prometheus to object storage (Swift in this
case).

Thanos has an important role in this monitoring stack and contains four main components:
  * Thanos side car container: runs along with Prometheus and uploads the generated data 
  to object storage.
  * Thanos store: Fetches and unpacks long term monitoring data from object storage.
  * Thanos query: Queries Thanos store for long term data and exposes using an identical
  API to the Prometheus API.
  * Thanos compactor: Connects to object storage in order to lower the resolution of 
  old monitoring data.

Grafana is configured with data sources for both Prometheus (short term data)
and Thanos query (long term data). Some Grafana dashboards are also added that use
these data sources and provide data on the OpenShift cluster's state.

## Configuring the monitoring stack

This monitoring stack can be either configured via the role variables (defined under: 
playbooks/roles/cluster_monitoring/defaults/main.yml) or via directly modifying the
relevant objects definition.

The cluster_monitoring role has the following variables: 

   * **monitoring_namespace** The namespace which will run the monitoring stack.
   * **prometheus_operator_service_account** The service account name for the Prometheus 
   Operator.
   * **prometheus_service_account** The service account name for Prometheus.
   * **prometheus_custom_label** Prometheus's custom label.
   * **prometheus_oauth_proxy_memory_requests** Prometheus oAuth proxy container memory 
   request.
   * **prometheus_oauth_proxy_cpu_requests** Prometheus oAuth proxy container cpu request.
   * **prometheus_oauth_proxy_memory_limit** Prometheus oAuth proxy container memory limit.
   * **prometheus_oauth_proxy_cpu_limit** Prometheus oAuth proxy container cpu limit.
   * **prometheus_nodeselector** Node selector for Prometheus.
   * **prometheus_storage_class** Which storage class to use in order to persist prometheus
   data.
   * **prometheus_pvc_size** Persistent volume claim size for Prometheus data.
   * **prometheus_retention_time** How long does Prometheus retain the data for.
   * **grafana_service_account** Grafana service account name.
   * **grafana_memory_requests** Grafana oAuth proxy container memory request.
   * **grafana_memory_limit** Grafana oAuth proxy container memory limit.
   * **grafana_cpu_requests** Grafana oAuth proxy container cpu request.
   * **grafana_cpu_limit** Grafana oAuth proxy container cpu limit.
   * **thanos_image** Thanos image repository URL.
   * **thanos_image_version** Thanos's image tag.
   * **thanos_service_account** Thanos service account name.
   * **default_image_pull_policy** Image pull policy for the various deployments.


