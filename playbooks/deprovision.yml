---
- hosts: localhost
  gather_facts: no
  connection: local
  roles:
    - poc_facts
  tasks:
    - name: make sure we are allowed to deprovision
      assert:
        that:
          - "{{ deprovision_allowed|default(False) }} == True"
        fail_msg: "Must set deprovision_allowed to True to deprovision"

- name: Get info on volumes to delete
  gather_facts: no
  hosts: masters
  run_once: yes
  tasks:
    - name: Check if SSH works
      shell: ssh -o ConnectTimeout=10 {{ cluster_name }}-master-1 'echo success'
      register: ssh_result
      failed_when: false
      delegate_to: localhost

    - when: ('success' in ssh_result.stdout)
      block:
      - name: get names of dynamically provisioned cinder volumes
        shell: oc get pv -o jsonpath='{.items[?(@.spec.csi.driver)].metadata.name}'
        register: volume_name_result
        failed_when: false

      - name: put volume names in a list
        set_fact:
          cinder_volume_names: "{{ volume_name_result.stdout.split() }}"
        when: volume_name_result.rc == 0

      - name: show volumes to delete
        debug:
          var: cinder_volume_names
        when: volume_name_result.rc == 0

- hosts: localhost
  gather_facts: no
  connection: local
  roles:
    - poc_facts
  tasks:

    - name: Disassociate floating IP from bastion
      os_floating_ip:
        server: "{{ cluster_name }}-bastion"
        floating_ip_address: "{{ bastion_public_ip }}"
        state: absent
      failed_when: false

    - name: Disassociate floating IP from first LB node
      os_floating_ip:
        server: "{{ cluster_name }}-lb-1"
        floating_ip_address: "{{ openshift_public_ip }}"
        state: absent
      failed_when: false
      when: master_vm_group_size > 1

    - name: Disassociate floating IP from first master node
      os_floating_ip:
        server: "{{ cluster_name }}-master-1"
        floating_ip_address: "{{ openshift_public_ip }}"
        state: absent
      failed_when: false
      when: master_vm_group_size == 1

    - name: Delete public key
      os_keypair:
        state: absent
        name: "{{ cluster_name }}"

    - name: Delete compute nodes stacks
      async: 300
      poll: 0
      register: async_job_compute
      os_stack:
        name: "{{ cluster_name }}-{{ item.stack_name }}"
        state: absent
        wait: yes
      with_items: "{{ compute_node_groups }}"

    - name: Delete infra nodes stacks
      async: 300
      poll: 0
      register: async_job_infra
      os_stack:
        name: "{{ cluster_name }}-{{ item.stack_name }}"
        state: absent
        wait: yes
      with_items: "{{ infra_node_groups|default([]) }}"

    - name: Delete gpu nodes stacks
      async: 300
      poll: 0
      register: async_job_gpu
      os_stack:
        name: "{{ cluster_name }}-{{ item.stack_name }}"
        state: absent
        wait: yes
      with_items: "{{ gpu_node_groups|default([]) }}"

    - name: Delete stack {{ cluster_name }}-etcd (multimaster)
      async: 300
      poll: 0
      register: async_job_etcd
      os_stack:
        name: "{{ cluster_name }}-etcd"
        state: absent
        wait: yes

    - name: Delete stack {{ cluster_name }}-cluster
      async: 300
      poll: 0
      register: async_job_cluster
      os_stack:
        name: "{{ cluster_name }}-cluster"
        state: absent
        wait: yes

    - name: Delete stack {{ cluster_name }}-glusterfs
      async: 300
      poll: 0
      register: async_job_glusterfs
      os_stack:
        name: "{{ cluster_name }}-glusterfs"
        state: absent
        wait: yes

    - name: Delete stack {{ cluster_name }}-egress-ips
      async: 300
      poll: 0
      register: async_job_egress
      os_stack:
        name: "{{ cluster_name }}-egress-ips"
        state: absent
        wait: yes

    - name: Delete stack {{ cluster_name }}-base
      async: 300
      poll: 0
      register: async_job_base
      os_stack:
        name: "{{ cluster_name }}-base"
        state: absent
        wait: yes

    # Wait for async jobs to finish
    # Steps that used with_items have to use with_items here as well
    - name: Wait for async deletion to complete (compute)
      async_status: jid="{{ item.ansible_job_id }}"
      register: async_poll_compute
      until: async_poll_compute.finished
      retries: 30
      delay: 3
      with_items: "{{ async_job_compute.results }}"

    - name: Wait for async deletion to complete (infra)
      async_status: jid="{{ item.ansible_job_id }}"
      register: async_poll_infra
      until: async_poll_infra.finished
      retries: 30
      delay: 3
      with_items: "{{ async_job_infra.results }}"

    - name: Wait for async deletion to complete (gpu)
      async_status: jid="{{ item.ansible_job_id }}"
      register: async_poll_gpu
      until: async_poll_gpu.finished
      retries: 30
      delay: 3
      with_items: "{{ async_job_gpu.results }}"

    - name: Wait for async deletion to complete (etcd)
      async_status: jid="{{ async_job_etcd.ansible_job_id }}"
      register: async_poll_etcd
      until: async_poll_etcd.finished
      retries: 30
      delay: 3

    - name: Wait for async deletion to complete (cluster)
      async_status: jid="{{ async_job_cluster.ansible_job_id }}"
      register: async_poll_cluster
      until: async_poll_cluster.finished
      retries: 30
      delay: 3
    
    - name: Wait for async deletion to complete (glusterfs)
      async_status: jid="{{ async_job_glusterfs.ansible_job_id }}"
      register: async_poll_glusterfs
      until: async_poll_glusterfs.finished
      retries: 30
      delay: 3

    - name: Wait for async deletion to complete (egress-ips)
      async_status: jid="{{ async_job_egress.ansible_job_id }}"
      register: async_poll_egress
      until: async_poll_egress.finished
      retries: 30
      delay: 3
    
    - name: Wait for async deletion to complete (base)
      async_status: jid="{{ async_job_base.ansible_job_id }}"
      register: async_poll_base
      until: async_poll_base.finished
      retries: 30
      delay: 3

    # Async tasks completed, continue with the rest
    - name: Delete the integrated registry object storage container
      shell: openstack container delete {{ openshift_hosted_registry_storage_swift_container }} --recursive
      register: swift_output
      changed_when: swift_output.rc == 0
      ignore_errors: yes

    - name: Delete dynamically created Cinder volumes
      os_volume:
        display_name: "{{ item }}"
        state: absent
      with_items: "{{ hostvars[groups['masters'][0]]['cinder_volume_names']|default([]) }}"
      when:
      - groups['masters'] | length > 0
      - ('success' in hostvars[groups['masters'][0]]['ssh_result'].stdout)

    - when:
      - groups['masters'] | length > 0
      block:
      - name: Assert that SSH connections to masters worked
        assert:
          that:
            - ('success' in hostvars[groups['masters'][0]]['ssh_result'].stdout)
          fail_msg: >
            SSH connection to master nodes failed during deprovisioning.
            There might be some leftover Cinder volumes which need to be deleted manually.
            Please check the list of volumes by running: "openstack volume list", and
            manually deleting them with: "openstack volume delete <VOLUME_NAME>"

      - name: Assert the potential of having leftover cinder volumes
        # Volume deletion may take a while so retry few times
        retries: 10
        delay: 3
        until: hostvars[groups['masters'][0]]['volume_name_result'].rc == 0
        assert:
          that:
            - hostvars[groups['masters'][0]]['volume_name_result'].rc == 0
          fail_msg: >
            API query on master nodes did not work during deprovisioning.
            There might be some leftover Cinder volumes which need to be deleted manually.
            Please check the list of volumes by running: "openstack volume list", and
            manually deleting them with: "openstack volume delete <VOLUME_NAME>"
      always:
      - name: Remove inventory cache
        file:
          path: "{{ lookup('env', 'HOME') }}/.cache/openstack/ansible-inventory.cache"
          state: absent
