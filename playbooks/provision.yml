---
- hosts: localhost
  gather_facts: no
  connection: local
  roles:
    - poc_facts
  tasks:
    - name: Add public key to OpenStack for {{ cluster_name }}
      os_keypair:
        state: present
        name: "{{ cluster_name }}"
        public_key_file: "/dev/shm/{{ cluster_name }}/id_rsa.pub"

    - name: Register stack creation status (base)
      shell: openstack stack environment show -f json {{ cluster_name }}-base
      register: stack_output_base
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (etcd)
      shell: openstack stack environment show -f json {{ cluster_name }}-etcd
      register: stack_output_etcd
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (cluster)
      shell: openstack stack environment show -f json {{ cluster_name }}-cluster
      register: stack_output_cluster
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (glusterfs)
      shell: openstack stack environment show -f json {{ cluster_name }}-glusterfs
      register: stack_output_glusterfs
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - block:
      - name: Format output from GlusterFS Heat stack environment as JSON
        set_fact:
          stack_output_glusterfs_json: "{{ stack_output_glusterfs.stdout | from_json }}"

      - name: Put current number of GlusterFS nodes into a variable
        set_fact:
          glusterfs_vm_group_size_now: "{{ stack_output_glusterfs_json.parameters.glusterfs_vm_group_size }}"

      - name: Put current size of GlusterFS extension volume group into a variable
        set_fact:
          glusterfs_extension_volume_group_size_now: "{{ stack_output_glusterfs_json.parameters.glusterfs_extension_volume_group_size|default(0) }}"

      - name: Assert that we are not scaling down the number of GlusterFS nodes
        assert:
          that:
            - glusterfs_vm_group_size|int >= glusterfs_vm_group_size_now|int
          msg: >
            Can't scale down the number of GlusterFS nodes
            (glusterfs_vm_group_size) as this could lead to data loss.
            Please set assert_no_gluster_nodes_scaledown to false if you really
            want to do this.
        when: assert_no_gluster_nodes_scaledown|default(True)|bool

      - name: Assert that we are not shrinking the number of GlusterFS volumes
        assert:
          that:
            - glusterfs_extension_volume_group_size|default(0)|int >= glusterfs_extension_volume_group_size_now|int
          msg: >
            Can't reduce glusterfs_extension_volume_group_size as this could lead
            to data loss. Please set assert_no_gluster_vol_scaledown to false if
            you really want to do this.
        when: assert_no_gluster_vol_scaledown|default(True)|bool
      # end block
      when: stack_output_glusterfs.stderr.find('Stack not found') == -1

    - name: Register stack creation status (compute nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_compute_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ compute_node_groups }}"
      tags:
        - skip_ansible_lint

    - name: Optionally assert that this is a new deployment (control plane stacks)
      assert:
        that:
          - stack_output_base.stderr.find('Stack not found') != -1
          - stack_output_cluster.stderr.find('Stack not found') != -1
          - stack_output_glusterfs.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, control plane stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool

    - name: Optionally assert that this is a new deployment (compute node stacks)
      assert:
        that:
          - item.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, compute node stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool
      with_items: "{{ stack_output_compute_nodes.results }}"

    - name: Create security group rules for API access
      set_fact:
        api_fw_rules: >
          {{
            api_fw_rules|default([]) + [
              {
                'protocol': 'tcp',
                'port_range_min': item[1],
                'port_range_max': item[1],
                'remote_ip_prefix': item[0]
              }
            ]
          }}
      with_nested:
        - "{{ ip_whitelist_api }}"
        - [ 8443 ]
      no_log: true

    - name: Create security group rules for load balancer access
      set_fact:
        lb_fw_rules: >
          {{
            lb_fw_rules|default([]) + [
              {
                'protocol': 'tcp',
                'port_range_min': item[1],
                'port_range_max': item[1],
                'remote_ip_prefix': item[0]
              }
            ]
          }}
      with_nested:
        - "{{ ip_whitelist_lb }}"
        - [ 80, 443 ]
      no_log: true

    - name: Combine external secgroup rules
      set_fact:
        secgroup_ext_access_rules: "{{ api_fw_rules + lb_fw_rules + extra_secgroup_ext_access_rules|default([]) }}"

    - name: Build/update the OpenShift Heat stack (base)
      register: heat_stack_base
      os_stack:
        name: "{{ cluster_name }}-base"
        state: present
        template: "files/openshift-heat-stack-base.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          secgroup_ext_access_rules: "{{ secgroup_ext_access_rules }}"
          openshift_network_cidr: "{{ openshift_network_cidr }}"
          openshift_network_allocation_pool_start: "{{ openshift_network_allocation_pool_start }}"
          openshift_network_allocation_pool_end: "{{ openshift_network_allocation_pool_end }}"
          openshift_router: "{{ openshift_router }}"
          bastion_vm_image: "{{ bastion_vm_image }}"
          bastion_vm_flavor: "{{ bastion_vm_flavor }}"
          bastion_cloud_config: "{{ bastion_cloud_config|default({}) }}"
          bastion_allow_cidrs: "{{ ip_whitelist_bastion }}"
          bastion_allow_ports: "{{ bastion_allow_ports }}"

    - name: Put base stack output into a dict
      set_fact:
        base_stack_outputs: "{{ heat_stack_base.stack.outputs[0].output_value }}"

    - name: Build/update the OpenShift Heat stack (multimaster - etcd)
      os_stack:
        name: "{{ cluster_name }}-etcd"
        state: present
        template: "files/openshift-heat-stack-etcd.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          etcd_vm_group_size: "{{ etcd_vm_group_size }}"
          etcd_vm_image: "{{ etcd_vm_image }}"
          etcd_vm_flavor: "{{ etcd_vm_flavor }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          network_id: "{{ base_stack_outputs.network_id}}"
          network_prefix: "{{ openshift_network_prefix }}"
          resource_group_identifier: "{{ etcd_resource_group_identifier }}"
      when:
        - master_vm_group_size > 1
        - stack_output_etcd.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_etcd|default(false)|bool

    - block:
      - name: Get id of LB VIP floating IP
        command: openstack floating ip show -f value -c id {{ openshift_public_ip }}
        register: cmd_lb_vip_floatingip_id

      - name: Build/update the OpenShift Heat stack (cluster/multimaster)
        os_stack:
          name: "{{ cluster_name }}-cluster"
          state: present
          template: "files/openshift-heat-stack-cluster.yml"
          wait: yes
          parameters:
            env_name: "{{ cluster_name }}"
            key_name: "{{ cluster_name }}"
            secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
            secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
            secgroup_id_ext_access: "{{ base_stack_outputs.secgroup_id_ext_access }}"
            network_id: "{{ base_stack_outputs.network_id }}"
            subnet_id: "{{ base_stack_outputs.subnet_id }}"
            network_prefix: "{{ openshift_network_prefix }}"
            lb_vm_group_size: "{{ lb_vm_group_size }}"
            lb_vm_image: "{{ lb_vm_image }}"
            lb_vm_flavor: "{{ lb_vm_flavor }}"
            lb_vol_size: "{{ lb_vol_size }}"
            lb_vip_floatingip_id: "{{ cmd_lb_vip_floatingip_id.stdout }}"
            nfs_vm_group_size: "{{ nfs_vm_group_size }}"
            nfs_vm_image: "{{ nfs_vm_image }}"
            nfs_vm_flavor: "{{ nfs_vm_flavor }}"
            nfs_vol_size: "{{ nfs_vol_size }}"
            master_vm_group_size: "{{ master_vm_group_size }}"
            master_vm_image: "{{ master_vm_image }}"
            master_vm_flavor: "{{ master_vm_flavor }}"
            lb_resource_group_identifier: "{{ lb_resource_group_identifier }}"
            lb_vip_ip: "{{ lb_vip_ip }}"
            master_resource_group_identifier: "{{ master_resource_group_identifier }}"
            nfs_resource_group_identifier: "{{ nfs_resource_group_identifier }}"
      when:
        - master_vm_group_size > 1
        - stack_output_cluster.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_cluster|default(false)|bool

    - name: Build/update the OpenShift Heat stack (cluster/singlemaster)
      register: heat_stack
      os_stack:
        name: "{{ cluster_name }}-cluster"
        state: present
        template: "files/openshift-heat-stack-minimal.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          secgroup_id_ext_access: "{{ base_stack_outputs.secgroup_id_ext_access }}"
          network_id: "{{ base_stack_outputs.network_id }}"
          network_prefix: "{{ openshift_network_prefix }}"
          master_vm_group_size: "{{ master_vm_group_size }}"
          master_vm_image: "{{ master_vm_image }}"
          master_vm_flavor: "{{ master_vm_flavor }}"
          master_vm_vol_size: "{{ master_vm_vol_size }}"
          master_resource_group_identifier: "{{ master_resource_group_identifier }}"
      when:
        - master_vm_group_size == 1
        - stack_output_cluster.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_cluster|default(false)|bool

    - name: Build/update the OpenShift Heat stack (glusterfs)
      os_stack:
        name: "{{ cluster_name }}-glusterfs"
        state: present
        template: "files/openshift-heat-stack-glusterfs.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          glusterfs_vm_group_size: "{{ glusterfs_vm_group_size }}"
          glusterfs_vm_image: "{{ glusterfs_vm_image }}"
          glusterfs_vm_flavor: "{{ glusterfs_vm_flavor }}"
          glusterfs_vol_1_size: "{{ glusterfs_docker_vol_size|default('10') }}"
          glusterfs_vol_2_size: "{{ glusterfs_vol_size }}"
          glusterfs_extension_volume_group_size: "{{ glusterfs_extension_volume_group_size|default(0) }}"
          glusterfs_extension_volume_size: "{{ glusterfs_extension_volume_size|default(0) }}"
          glusterfs_volume_deletion_policy: "{{ glusterfs_volume_deletion_policy|default('Retain') }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          network_id: "{{ base_stack_outputs.network_id }}"
          network_prefix: "{{ openshift_network_prefix }}"
          resource_group_identifier: "{{ glusterfs_resource_group_identifier }}"
      when:
        - stack_output_glusterfs.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_glusterfs|default(false)|bool

    - name: Build/update compute node stack(s)
      os_stack:
        name: "{{ cluster_name }}-{{ item.stack_name }}"
        state: present
        template: "files/openshift-heat-stack-compute-nodes.yml"
        wait: yes
        parameters: "{{ item.heat_parameters | combine(base_stack_outputs) }}"
      with_items: "{{ compute_node_groups }}"
      when:
        - stack_output_base.stderr.find('Stack not found') != -1 or
          item.stack_name in allow_heat_stack_update_node_groups|default([])

    - name: Remove inventory cache
      file:
        path: "{{ lookup('env', 'HOME') }}/.cache/openstack/ansible-inventory.cache"
        state: absent

    - name: Refresh dynamic inventory
      meta: refresh_inventory

    - name: Associate floating IP with bastion host
      os_floating_ip:
        server: "{{ cluster_name }}-bastion"
        floating_ip_address: "{{ bastion_public_ip }}"

    - name: Associate floating IP with singlemaster
      os_floating_ip:
        server: "{{ cluster_name }}-master-1"
        floating_ip_address: "{{ openshift_public_ip }}"
      when:
        - master_vm_group_size == 1
        - skip_public_ip_association|default(false)|bool == false

    - name: Assign extra floating IPs
      os_floating_ip:
        server: "{{ item.assign_to }}"
        floating_ip_address: "{{ item.address }}"
      with_items: "{{ ip_address_data|default([]) }}"
      when:
        - item.assign_to is defined

    - name: Refresh dynamic inventory
      meta: refresh_inventory

- include: generate_ssh_config.yml

- hosts: localhost
  gather_facts: no
  connection: local
  tasks:
    - name: Wait for connectivity on port 22 on the bastion
      wait_for:
        host: "{{ bastion_public_ip }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 900

    - name: Wait for SSH to work
      shell: ssh {{ bastion_public_ip }} 'echo success'
      register: result
      until: result.stdout.find('success') != -1
      retries: 30
      delay: 5
      changed_when: false

- name: Install nmap-ncat on bastion if need be
  hosts: bastion
  gather_facts: no
  become: yes
  tasks:
    - name: Install nmap-ncat
      yum:
        name: nmap-ncat
        state: present

- name: Wait for SSH to work on all other hosts
  hosts: all
  gather_facts: no
  any_errors_fatal: yes
  tasks:
    - name: Wait for connectivity on port 22 on all other hosts
      wait_for:
        host: "{{ ansible_ssh_host }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 300
      delegate_to: "{{ hostvars[groups['bastion'][0]]['ansible_ssh_host'] }}"
      when: check_for_ssh_after_provisioning|default(false)|bool

- import_playbook: get_install_state.yml

- name: Update packages on cluster hosts
  hosts: OSEv3
  tasks:
    - when:
        - poc_update_and_reboot|default(false)|bool
        - poc_installed|default(false)|bool == false
      block:
        - name: update OS packages
          yum:
            name: '*'
            state: latest
          register: os_packages_updated

        - name: reboot hosts
          shell: ( /bin/sleep 5 ; shutdown -r now "Ansible triggered reboot" ) &
          async: 30
          poll: 0
          ignore_errors: true
          when: os_packages_updated.changed

        - name: wait for hosts to go down
          wait_for:
            state: stopped
            host: "{{ ansible_ssh_host }}"
            port: 22
            timeout: 300
          delegate_to: "{{ hostvars[groups['bastion'][0]]['ansible_ssh_host'] }}"
          when: os_packages_updated.changed

        # meta-action for reseting control master connections would be used, but it does not work currently
        # https://github.com/ansible/ansible/issues/27520
        - name: wait for control master processes to time out
          pause: seconds=30
          when: os_packages_updated.changed

        - name: wait for connectivity on port 22 on hosts
          shell: ssh {{ ansible_ssh_host }} 'echo success'
          register: result
          until: result.stdout.find('success') != -1
          retries: 30
          delay: 10
          changed_when: false
          delegate_to: localhost
          when: os_packages_updated.changed
      #endblock
