---
- hosts: localhost
  gather_facts: no
  connection: local
  roles:
    - poc_facts
  tasks:
    - name: Add public key to OpenStack for {{ cluster_name }}
      os_keypair:
        state: present
        name: "{{ cluster_name }}"
        public_key_file: "/dev/shm/{{ cluster_name }}/id_rsa.pub"

    - name: Register stack creation status (base)
      shell: openstack stack environment show -f json {{ cluster_name }}-base
      register: stack_output_base
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (etcd)
      shell: openstack stack environment show -f json {{ cluster_name }}-etcd
      register: stack_output_etcd
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (cluster)
      shell: openstack stack environment show -f json {{ cluster_name }}-cluster
      register: stack_output_cluster
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (glusterfs)
      shell: openstack stack environment show -f json {{ cluster_name }}-glusterfs
      register: stack_output_glusterfs
      failed_when: false
      changed_when: false
      no_log: True
      tags:
        - skip_ansible_lint
        -
    - name: Register stack creation status (infra nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_infra_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ infra_node_groups|default([]) }}"
      tags:
        - skip_ansible_lint
        -
    - name: Register stack creation status (compute nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_compute_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ compute_node_groups }}"
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (gpu nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_gpu_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ gpu_node_groups }}"
      tags:
        - skip_ansible_lint
      when: gpu_node_groups is defined

    - block:
      - name: Format output from GlusterFS Heat stack environment as JSON
        set_fact:
          stack_output_glusterfs_json: "{{ stack_output_glusterfs.stdout | from_json }}"

      - name: Put current number of GlusterFS nodes into a variable
        set_fact:
          glusterfs_vm_group_size_now: "{{ stack_output_glusterfs_json.parameters.glusterfs_vm_group_size }}"

      - name: Put current size of GlusterFS extension volume group into a variable
        set_fact:
          glusterfs_extension_volume_group_size_now: "{{ stack_output_glusterfs_json.parameters.glusterfs_extension_volume_group_size|default(0) }}"

      - name: Assert that we are not scaling down the number of GlusterFS nodes
        assert:
          that:
            - glusterfs_vm_group_size|int >= glusterfs_vm_group_size_now|int
          msg: >
            Can't scale down the number of GlusterFS nodes
            (glusterfs_vm_group_size) as this could lead to data loss.
            Please set assert_no_gluster_nodes_scaledown to false if you really
            want to do this.
        when: assert_no_gluster_nodes_scaledown|default(True)|bool

      - name: Assert that we are not shrinking the number of GlusterFS volumes
        assert:
          that:
            - glusterfs_extension_volume_group_size|default(0)|int >= glusterfs_extension_volume_group_size_now|int
          msg: >
            Can't reduce glusterfs_extension_volume_group_size as this could lead
            to data loss. Please set assert_no_gluster_vol_scaledown to false if
            you really want to do this.
        when: assert_no_gluster_vol_scaledown|default(True)|bool
      # end block
      when: stack_output_glusterfs.stderr.find('Stack not found') == -1

    - name: Register stack creation status (infra nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_infra_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ infra_node_groups|default([]) }}"
      tags:
        - skip_ansible_lint

    - name: Register stack creation status (compute nodes)
      shell: openstack stack environment show -f json {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_compute_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ compute_node_groups }}"
      tags:
        - skip_ansible_lint

    - name: Optionally assert that this is a new deployment (control plane stacks)
      assert:
        that:
          - stack_output_base.stderr.find('Stack not found') != -1
          - stack_output_cluster.stderr.find('Stack not found') != -1
          - stack_output_glusterfs.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, control plane stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool

    - name: Optionally assert that this is a new deployment (compute node stacks)
      assert:
        that:
          - item.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, compute node stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool
      with_items: "{{ stack_output_compute_nodes.results }}"

    - name: Create security group rules for API access
      set_fact:
        api_fw_rules: >
          {{
            api_fw_rules|default([]) + [
              {
                'protocol': 'tcp',
                'port_range_min': item[1],
                'port_range_max': item[1],
                'remote_ip_prefix': item[0]
              }
            ]
          }}
      with_nested:
        - "{{ ip_whitelist_api }}"
        - [ 8443 ]
      no_log: true

    - name: Create security group rules for load balancer access
      set_fact:
        lb_fw_rules: >
          {{
            lb_fw_rules|default([]) + [
              {
                'protocol': 'tcp',
                'port_range_min': item[1],
                'port_range_max': item[1],
                'remote_ip_prefix': item[0]
              }
            ]
          }}
      with_nested:
        - "{{ ip_whitelist_lb }}"
        - [ 80, 443 ]
      no_log: true

    - name: Combine external secgroup rules
      set_fact:
        secgroup_ext_access_rules: "{{ api_fw_rules + lb_fw_rules + extra_secgroup_ext_access_rules|default([]) }}"

    - name: Provision resources with floating IP network
      include_tasks: tasks/provision_with_floating_ips.yml
      when:
        - poc_public_ip_network|default('') == ''

    - name: Provision resources with public IP network
      include_tasks: tasks/provision_with_public_ips.yml
      when:
        - poc_public_ip_network|default('') != ''

    - name: Remove inventory cache
      file:
        path: "{{ lookup('env', 'HOME') }}/.cache/openstack/ansible-inventory.cache"
        state: absent

    - name: Refresh dynamic inventory
      meta: refresh_inventory

    # Floating IP setup
    - when:
        - poc_public_ip_network|default('') == ''
      block:
        - name: Associate floating IP with bastion host
          os_floating_ip:
            server: "{{ cluster_name }}-bastion"
            floating_ip_address: "{{ bastion_public_ip }}"

        - name: Associate floating IP with singlemaster
          os_floating_ip:
            server: "{{ cluster_name }}-master-1"
            floating_ip_address: "{{ openshift_public_ip }}"
          when:
            - master_vm_group_size == 1
            - skip_public_ip_association|default(false)|bool == false

        - name: Assign extra floating IPs
          os_floating_ip:
            server: "{{ item.assign_to }}"
            floating_ip_address: "{{ item.address }}"
          with_items: "{{ ip_address_data|default([]) }}"
          when:
            - item.assign_to is defined
      # endblock

    - name: Refresh dynamic inventory
      meta: refresh_inventory

- include: generate_ssh_config.yml

- hosts: localhost
  gather_facts: no
  connection: local
  tasks:
    - name: Wait for connectivity on port 22 on the bastion
      wait_for:
        host: "{{ bastion_public_ip }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 900

    - name: Wait for SSH to work
      shell: ssh {{ bastion_public_ip }} 'echo success'
      register: result
      until: result.stdout.find('success') != -1
      retries: 30
      delay: 5
      changed_when: false

- name: Configure bastion
  hosts: bastion
  become: yes
  roles:
    - base
    - ansible-role-nrpe
    - ansible-role-nrpe-plugins
    - bastion
    - ansible-role-mod_gearman

- name: Wait for SSH to work on all other hosts
  hosts: all
  gather_facts: no
  any_errors_fatal: yes
  tasks:
    - name: Wait for connectivity on port 22 on all other hosts
      wait_for:
        host: "{{ ansible_ssh_host }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 300
      delegate_to: "{{ hostvars[groups['bastion'][0]]['ansible_ssh_host'] }}"
      when: check_for_ssh_after_provisioning|default(false)|bool

- name: Gather facts on all hosts (needed for cluster_common on bastion)
  hosts: all
  become: yes
  gather_facts: no
  tasks:
   - setup:
       filter: 'ansible_*'

- name: Configure cluster common parts on bastion
  hosts: bastion
  become: yes
  roles:
    - cluster_common

- import_playbook: get_install_state.yml

- name: Update packages on cluster hosts
  hosts: OSEv3
  tasks:
    - when:
        - poc_update_and_reboot|default(false)|bool
        - poc_installed|default(false)|bool == false
      block:
        - name: update OS packages
          yum:
            name: '*'
            state: latest
          register: os_packages_updated

        - name: reboot hosts
          shell: ( /bin/sleep 5 ; shutdown -r now "Ansible triggered reboot" ) &
          async: 30
          poll: 0
          ignore_errors: true
          when: os_packages_updated.changed

        - name: wait for hosts to go down
          wait_for:
            state: stopped
            host: "{{ ansible_ssh_host }}"
            port: 22
            timeout: 300
          delegate_to: "{{ hostvars[groups['bastion'][0]]['ansible_ssh_host'] }}"
          when: os_packages_updated.changed

        # meta-action for reseting control master connections would be used, but it does not work currently
        # https://github.com/ansible/ansible/issues/27520
        - name: wait for control master processes to time out
          pause: seconds=30
          when: os_packages_updated.changed

        - name: wait for connectivity on port 22 on hosts
          shell: ssh {{ ansible_ssh_host }} 'echo success'
          register: result
          until: result.stdout.find('success') != -1
          retries: 30
          delay: 10
          changed_when: false
          delegate_to: localhost
          when: os_packages_updated.changed
      #endblock
