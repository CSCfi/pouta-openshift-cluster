---
- hosts: localhost
  gather_facts: no
  connection: local
  tasks:
    - include: environment_context.yml

    - name: Add public key to OpenStack for {{ cluster_name }}
      os_keypair:
        state: present
        name: "{{ cluster_name }}"
        public_key_file: "/dev/shm/{{ cluster_name }}/id_rsa.pub"

    - name: Register stack creation status (base)
      shell: openstack stack show {{ cluster_name }}-base
      register: stack_output_base
      failed_when: false
      changed_when: false
      no_log: True

    - name: Register stack creation status (etcd)
      shell: openstack stack show {{ cluster_name }}-etcd
      register: stack_output_etcd
      failed_when: false
      changed_when: false
      no_log: True

    - name: Register stack creation status (cluster)
      shell: openstack stack show {{ cluster_name }}-cluster
      register: stack_output_cluster
      failed_when: false
      changed_when: false
      no_log: True

    - name: Register stack creation status (glusterfs)
      shell: openstack stack show {{ cluster_name }}-glusterfs
      register: stack_output_glusterfs
      failed_when: false
      changed_when: false
      no_log: True

    - name: Register stack creation status (compute nodes)
      shell: openstack stack show {{ cluster_name }}-{{ item.stack_name }}
      register: stack_output_compute_nodes
      failed_when: false
      changed_when: false
      no_log: True
      with_items: "{{ compute_node_groups }}"

    - name: Optionally assert that this is a new deployment (control plane stacks)
      assert:
        that:
          - stack_output_base.stderr.find('Stack not found') != -1
          - stack_output_cluster.stderr.find('Stack not found') != -1
          - stack_output_glusterfs.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, control plane stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool

    - name: Optionally assert that this is a new deployment (compute node stacks)
      assert:
        that:
          - item.stderr.find('Stack not found') != -1
        msg: >
          Asserting a new clean deployment failed, compute node stack(s) exist.
          Since we require a clean deployment, this is probably a CI environment
          and there are some leftover resources from a previous CI build.
          Please clean up the old build and try again.
      when: assert_new_deployment|default(False)|bool
      with_items: "{{ stack_output_compute_nodes.results }}"

    - name: Build/update the OpenShift Heat stack (base)
      register: heat_stack_base
      os_stack:
        name: "{{ cluster_name }}-base"
        state: present
        template: "files/openshift-heat-stack-base.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          secgroup_ext_access_rules: "{{ secgroup_ext_access_rules }}"
          openshift_network_cidr: "{{ openshift_network_cidr }}"
          openshift_network_allocation_pool_start: "{{ openshift_network_allocation_pool_start }}"
          openshift_network_allocation_pool_end: "{{ openshift_network_allocation_pool_end }}"
          openshift_router: "{{ openshift_router }}"
          bastion_vm_image: "{{ bastion_vm_image }}"
          bastion_vm_flavor: "{{ bastion_vm_flavor }}"
          bastion_cloud_config: "{{ bastion_cloud_config|default({}) }}"
          bastion_allow_cidrs: "{{ bastion_allow_cidrs }}"
          bastion_allow_ports: "{{ bastion_allow_ports }}"

    - name: Put base stack output into a dict
      set_fact:
        base_stack_outputs: "{{ heat_stack_base.stack.outputs[0].output_value }}"

    - name: Build/update the OpenShift Heat stack (multimaster - etcd)
      os_stack:
        name: "{{ cluster_name }}-etcd"
        state: present
        template: "files/openshift-heat-stack-etcd.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          etcd_vm_group_size: "{{ etcd_vm_group_size }}"
          etcd_vm_image: "{{ etcd_vm_image }}"
          etcd_vm_flavor: "{{ etcd_vm_flavor }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          network_id: "{{ base_stack_outputs.network_id}}"
          network_prefix: "{{ openshift_network_prefix }}"
          resource_group_identifier: "{{ etcd_resource_group_identifier }}"
      when:
        - master_vm_group_size > 1
        - stack_output_etcd.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_etcd|default(false)|bool

    - name: Build/update the OpenShift Heat stack (cluster/multimaster)
      os_stack:
        name: "{{ cluster_name }}-cluster"
        state: present
        template: "files/openshift-heat-stack-cluster.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          secgroup_id_ext_access: "{{ base_stack_outputs.secgroup_id_ext_access }}"
          network_id: "{{ base_stack_outputs.network_id }}"
          network_prefix: "{{ openshift_network_prefix }}"
          lb_vm_group_size: "{{ lb_vm_group_size }}"
          lb_vm_image: "{{ lb_vm_image }}"
          lb_vm_flavor: "{{ lb_vm_flavor }}"
          lb_vol_size: "{{ lb_vol_size }}"
          nfs_vm_group_size: "{{ nfs_vm_group_size }}"
          nfs_vm_image: "{{ nfs_vm_image }}"
          nfs_vm_flavor: "{{ nfs_vm_flavor }}"
          nfs_vol_size: "{{ nfs_vol_size }}"
          master_vm_group_size: "{{ master_vm_group_size }}"
          master_vm_image: "{{ master_vm_image }}"
          master_vm_flavor: "{{ master_vm_flavor }}"
          lb_resource_group_identifier: "{{ lb_resource_group_identifier }}"
          master_resource_group_identifier: "{{ master_resource_group_identifier }}"
          nfs_resource_group_identifier: "{{ nfs_resource_group_identifier }}"
      when:
        - master_vm_group_size > 1
        - stack_output_cluster.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_cluster|default(false)|bool

    - name: Build/update the OpenShift Heat stack (cluster/singlemaster)
      register: heat_stack
      os_stack:
        name: "{{ cluster_name }}-cluster"
        state: present
        template: "files/openshift-heat-stack-minimal.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          secgroup_id_ext_access: "{{ base_stack_outputs.secgroup_id_ext_access }}"
          network_id: "{{ base_stack_outputs.network_id }}"
          network_prefix: "{{ openshift_network_prefix }}"
          master_vm_group_size: "{{ master_vm_group_size }}"
          master_vm_image: "{{ master_vm_image }}"
          master_vm_flavor: "{{ master_vm_flavor }}"
          master_vm_vol_size: "{{ master_vm_vol_size }}"
          master_resource_group_identifier: "{{ master_resource_group_identifier }}"
      when:
        - master_vm_group_size == 1
        - stack_output_cluster.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_cluster|default(false)|bool

    - name: Build/update the OpenShift Heat stack (glusterfs)
      os_stack:
        name: "{{ cluster_name }}-glusterfs"
        state: present
        template: "files/openshift-heat-stack-glusterfs.yml"
        wait: yes
        parameters:
          env_name: "{{ cluster_name }}"
          key_name: "{{ cluster_name }}"
          glusterfs_vm_group_size: "{{ glusterfs_vm_group_size }}"
          glusterfs_vm_image: "{{ glusterfs_vm_image }}"
          glusterfs_vm_flavor: "{{ glusterfs_vm_flavor }}"
          glusterfs_vol_1_size: "{{ glusterfs_docker_vol_size|default('10') }}"
          glusterfs_vol_2_size: "{{ glusterfs_vol_size }}"
          secgroup_id_common: "{{ base_stack_outputs.secgroup_id_common }}"
          secgroup_id_infra: "{{ base_stack_outputs.secgroup_id_infra }}"
          network_id: "{{ base_stack_outputs.network_id }}"
          network_prefix: "{{ openshift_network_prefix }}"
          resource_group_identifier: "{{ glusterfs_resource_group_identifier }}"
      when:
        - stack_output_glusterfs.stderr.find('Stack not found') != -1 or
          allow_heat_stack_update_glusterfs|default(false)|bool

    - name: Build/update compute node stack(s)
      os_stack:
        name: "{{ cluster_name }}-{{ item.stack_name }}"
        state: present
        template: "files/openshift-heat-stack-compute-nodes.yml"
        wait: yes
        parameters: "{{ item.heat_parameters | combine(base_stack_outputs) }}"
      with_items: "{{ compute_node_groups }}"
      when:
        - stack_output_base.stderr.find('Stack not found') != -1 or
          item.stack_name in allow_heat_stack_update_node_groups|default([])

    - name: Remove inventory cache
      file:
        path: "{{ lookup('env', 'HOME') }}/.cache/openstack/ansible-inventory.cache"
        state: absent

    - name: Refresh dynamic inventory
      meta: refresh_inventory

    - name: Associate floating IP with master
      os_floating_ip:
        server: "{{ cluster_name }}-master-1"
        floating_ip_address: "{{ openshift_public_ip }}"
      when:
        - master_vm_group_size == 1
        - skip_public_ip_association|default(false)|bool == false

    - name: Associate floating IP with first LB node
      os_floating_ip:
        server: "{{ cluster_name }}-lb-1"
        floating_ip_address: "{{ openshift_public_ip }}"
      when:
        - master_vm_group_size > 1
        - skip_public_ip_association|default(false)|bool == false

    - name: Associate floating IP with bastion host
      os_floating_ip:
        server: "{{ cluster_name }}-bastion"
        floating_ip_address: "{{ bastion_public_ip }}"

    - name: Refresh dynamic inventory
      meta: refresh_inventory

- include: generate_ssh_config.yml

- hosts: localhost
  gather_facts: no
  connection: local
  tasks:
    - name: Wait for connectivity on port 22 on the bastion
      wait_for:
        host: "{{ bastion_public_ip }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 900
    - name: Wait for SSH to work
      shell: ssh {{ bastion_public_ip }} 'echo success'
      register: result
      until: result.stdout.find('success') != -1
      retries: 30
      delay: 5
      changed_when: false

- name: Install nmap-ncat on bastion if need be
  hosts: bastion
  gather_facts: no
  become: yes
  tasks:
    - name: Install nmap-ncat
      yum:
        name: nmap-ncat
        state: present

- name: Wait for SSH to work on all other hosts
  hosts: all
  gather_facts: no
  any_errors_fatal: yes
  tasks:
    - name: Wait for connectivity on port 22 on all other hosts
      wait_for:
        host: "{{ ansible_ssh_host }}"
        port: 22
        search_regex: "OpenSSH"
        delay: 5
        timeout: 300
      delegate_to: "{{ hostvars[groups['bastion'][0]]['ansible_ssh_host'] }}"
